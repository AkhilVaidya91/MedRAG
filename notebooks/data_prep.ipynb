{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2 datasets\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166123bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import io\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "from typing import Dict, List, Tuple\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"itsanmolgupta/mimic-cxr-dataset\"\n",
    "MODEL_NAME = \"google/medsiglip-448\"\n",
    "BATCH_SIZE = 16  # Adjust based on your GPU memory\n",
    "IMAGE_SIZE = 448  # MedSigLIP expects 448x448 images\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9869451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f795529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_tf_style(image: Image.Image, size: int = 448) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Resize image using TensorFlow's bilinear method to match MedSigLIP's\n",
    "    preprocessing pipeline from Big Vision library.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        size: Target size (default 448)\n",
    "    \n",
    "    Returns:\n",
    "        Resized PIL Image\n",
    "    \"\"\"\n",
    "    # Convert PIL to numpy array\n",
    "    img_array = np.array(image)\n",
    "    \n",
    "    # Use TensorFlow's resize (bilinear, no antialiasing)\n",
    "    resized = tf.image.resize(\n",
    "        images=img_array,\n",
    "        size=[size, size],\n",
    "        method='bilinear',\n",
    "        antialias=False\n",
    "    )\n",
    "    \n",
    "    # Convert back to PIL Image\n",
    "    return Image.fromarray(resized.numpy().astype(np.uint8))\n",
    "\n",
    "\n",
    "def image_to_base64(image: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Convert PIL Image to base64 string.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "    \n",
    "    Returns:\n",
    "        Base64 encoded string\n",
    "    \"\"\"\n",
    "    buffered = io.BytesIO()\n",
    "    # Save as PNG to preserve quality\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    img_bytes = buffered.getvalue()\n",
    "    return base64.b64encode(img_bytes).decode('utf-8')\n",
    "\n",
    "\n",
    "def process_batch(\n",
    "    batch: Dict,\n",
    "    processor,\n",
    "    model,\n",
    "    device: str\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Process a batch of data to generate embeddings and base64 images.\n",
    "    \n",
    "    Args:\n",
    "        batch: Dictionary containing images, findings, and impressions\n",
    "        processor: MedSigLIP processor\n",
    "        model: MedSigLIP model\n",
    "        device: Computing device (cuda/cpu)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (image_embeddings, text_embeddings, base64_images)\n",
    "    \"\"\"\n",
    "    # Resize images using TensorFlow-style resizing\n",
    "    resized_images = [resize_image_tf_style(img.convert(\"RGB\")) for img in batch['image']]\n",
    "    \n",
    "    # Convert images to base64 (original images, not resized)\n",
    "    base64_images = [image_to_base64(img) for img in batch['image']]\n",
    "    \n",
    "    # Combine findings and impression for text embeddings\n",
    "    # Handle None values in impression\n",
    "    texts = [\n",
    "        f\"Findings: {finding}. Impression: {impression if impression else 'None'}\"\n",
    "        for finding, impression in zip(batch['findings'], batch['impression'])\n",
    "    ]\n",
    "    \n",
    "    # Process images ONLY (no text)\n",
    "    image_inputs = processor(\n",
    "        images=resized_images,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate IMAGE embeddings only\n",
    "    with torch.no_grad():\n",
    "        # Get image embeddings\n",
    "        image_embeds = model.get_image_features(**image_inputs)\n",
    "    \n",
    "    # Process texts ONLY (no images) using the tokenizer\n",
    "    text_inputs = processor.tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate TEXT embeddings only\n",
    "    with torch.no_grad():\n",
    "        # Get text embeddings\n",
    "        text_embeds = model.get_text_features(**text_inputs)\n",
    "    \n",
    "    # Move embeddings to CPU\n",
    "    image_embeds = image_embeds.cpu().numpy()\n",
    "    text_embeds = text_embeds.cpu().numpy()\n",
    "    \n",
    "    return image_embeds, text_embeds, base64_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb24141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_model():\n",
    "    \"\"\"\n",
    "    Load the MIMIC-CXR dataset and MedSigLIP model.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (dataset, processor, model)\n",
    "    \"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "    print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "    \n",
    "    print(f\"\\nLoading MedSigLIP model from {MODEL_NAME}...\")\n",
    "    print(\"Note: You need to accept the model's terms of use on Hugging Face\")\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "    return dataset, processor, model\n",
    "\n",
    "# Load dataset and model\n",
    "# Uncomment the line below when ready to load\n",
    "dataset, processor, model = load_data_and_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca0ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset, processor, model, batch_size: int = BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Process entire dataset and generate embeddings.\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset\n",
    "        processor: MedSigLIP processor\n",
    "        model: MedSigLIP model\n",
    "        batch_size: Batch size for processing\n",
    "    \n",
    "    Returns:\n",
    "        List of processed records\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    num_batches = (len(dataset) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"\\nProcessing {len(dataset)} samples in {num_batches} batches...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=\"Processing batches\"):\n",
    "        batch_end = min(i + batch_size, len(dataset))\n",
    "        \n",
    "        try:\n",
    "            # Extract batch manually to ensure proper structure\n",
    "            batch = {\n",
    "                'image': [dataset[j]['image'] for j in range(i, batch_end)],\n",
    "                'findings': [dataset[j]['findings'] for j in range(i, batch_end)],\n",
    "                'impression': [dataset[j]['impression'] for j in range(i, batch_end)]\n",
    "            }\n",
    "            \n",
    "            # Process batch\n",
    "            image_embeds, text_embeds, base64_images = process_batch(\n",
    "                batch, processor, model, DEVICE\n",
    "            )\n",
    "            \n",
    "            # Prepare records for database insertion\n",
    "            for j in range(len(batch['findings'])):\n",
    "                record = {\n",
    "                    'findings': batch['findings'][j],\n",
    "                    'impression': batch['impression'][j],\n",
    "                    'image_base64': base64_images[j],\n",
    "                    'image_embedding': image_embeds[j].tolist(),\n",
    "                    'text_embedding': text_embeds[j].tolist()\n",
    "                }\n",
    "                all_records.append(record)\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if (i // batch_size) % 10 == 0:\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing batch starting at index {i}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nProcessing complete! Total records: {len(all_records)}\")\n",
    "    return all_records\n",
    "\n",
    "# Process the dataset\n",
    "# Uncomment when ready to process\n",
    "all_records = process_dataset(dataset, processor, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
